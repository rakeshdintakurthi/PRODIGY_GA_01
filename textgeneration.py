# -*- coding: utf-8 -*-
"""Textgeneration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J73butuDf4n0l3KmMu2Kk-CMgxtaI6oN
"""

!pip install -q git+https://github.com/huggingface/transformers.git
!pip install -q tensorflow==2.1

import tensorflow as tf
from transformers import TFGPT2LMHeadModel, GPT2Tokenizer


tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# add the EOS token as PAD token to avoid warnings
model = TFGPT2LMHeadModel.from_pretrained("gpt2", pad_token_id=tokenizer.eos_token_id)

# encode context the generation is conditioned on
input_ids = tokenizer.encode('A little girl', return_tensors='tf')

# generate text until the output length (which includes the context length) reaches 650
greedy_output = model.generate(input_ids, max_length=650)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))

# activate beam search and early_stopping
beam_output = model.generate(
    input_ids,
    max_length=50,
    num_beams=5,
    early_stopping=True
)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(beam_output[0], skip_special_tokens=True))